# Email Draft: ProT-VAE Authors

**Status:** NEEDS EMAIL RESEARCH

**Email:** (find via PNAS corresponding author)

---

## Recipient Research

| Field | Details |
|-------|---------|
| **Name** | (Corresponding author from PNAS) |
| **Position** | Paper Author |
| **Key Paper** | ProT-VAE (PNAS 2025) |
| **Recent Focus** | Transformer-based protein VAE |
| **Lab** | (via PNAS affiliation) |

---

## EMAIL

**To:** (via PNAS)

**Subject:** ProT-VAE architecture question - transformer vs. hyperbolic latent space?

---

(Use first name once identified),

Your PNAS paper on ProT-VAE combines transformers with VAEs nicely. We've taken a different route: MLP encoder but hyperbolic (Poincare) latent space instead of Euclidean.

Curious about a design choice: does ProT-VAE's transformer encoder already capture hierarchical structure through attention patterns, or does the flat latent space lose some of that information?

Our hyperbolic VAE achieves rho=-0.83 hierarchy correlation and rho=0.58 stability prediction without transformers. The geometry does heavy lifting that attention otherwise handles.

Question: would comparing latent space properties between transformer+Euclidean vs. MLP+hyperbolic reveal what each architecture captures?

Would a quick architecture comparison be interesting?

Best,
Ivan Weiss Van Der Pol
Jonathan Verdun
Kyrian Weiss Van Der Pol
AI Whisperers | CONACYT Paraguay
github.com/Ai-Whisperers/ternary-vaes-bioinformatics

---

## Notes

- **Word count:** 113
- **Why this works:** Specific architectural question, proposes concrete comparison
- **Tone:** Adjust once author identified

---

*Last Updated: 2026-01-26*
