# Email Draft: DLFea4AMPGen Authors

**Status:** NEEDS EMAIL RESEARCH

**Email:** (via Nature correspondence - check paper for corresponding author)

---

## Recipient Research

| Field | Details |
|-------|---------|
| **Name** | DLFea4AMPGen Authors |
| **Position** | Various institutions |
| **Key Paper** | Deep learning features for AMP generation (Nature 2025) |
| **Recent Focus** | Neural network feature extraction for peptides |
| **Innovation** | Novel learned representations |

---

## EMAIL

**To:** (corresponding author from paper)

**Subject:** Deep features vs. geometric features - what do each capture?

---

Hi,

Your DLFea4AMPGen work on learned features for AMP generation raises interesting questions about representation. We've been using hand-crafted geometric features (hyperbolic p-adic embeddings) and getting decent results (Spearman 0.66).

The philosophical question: do neural network features and geometric features capture the same information differently, or genuinely different information?

If the latter, combining them should improve predictions. If the former, the simpler approach wins.

We'd be curious to test this. If you have feature vectors available for a common benchmark set, we could check correlation between your deep features and our geometric features. High correlation = redundant. Low correlation = complementary.

Interested in a quick comparison?

Best,
Ivan Weiss Van Der Pol
Jonathan Verdun
Kyrian Weiss Van Der Pol
AI Whisperers | CONACYT Paraguay
github.com/Ai-Whisperers/ternary-vaes-bioinformatics

---

## Notes

- **Word count:** 125
- **Why this works:** Frames as intellectual question, proposes simple testable experiment
- **Tone:** Research peer, curious

---

*Last Updated: 2026-01-26*
