#!/usr/bin/env python3
"""Gradient-Enhanced DDG Prediction.

Uses the discovered DDG gradient direction (0.947 correlation) combined
with the MLP Refiner approach to achieve maximum performance.

Key insight: The gradient discovery showed that 94.7% of DDG variance
is explained by a single direction in the 32-dim latent space. We can
use this direction as a strong feature.
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.stats import spearmanr, pearsonr
from torch.utils.data import DataLoader, Dataset

import sys
sys.path.insert(0, str(Path(__file__).parents[3]))

from src.bioinformatics.models.ddg_vae import DDGVAE
from src.bioinformatics.data.protherm_loader import ProThermLoader
from src.bioinformatics.data.preprocessing import compute_features


@dataclass
class GradientConfig:
    """Configuration for gradient-enhanced prediction."""
    epochs: int = 200
    batch_size: int = 8
    learning_rate: float = 1e-4
    weight_decay: float = 1e-4
    patience: int = 40
    dropout: float = 0.2

    vae_checkpoint: str = "outputs/ddg_vae_training_20260129_212316/vae_protherm/best.pt"
    gradient_results: str = "outputs/gradient_discovery_20260129_231635/gradient_discovery_results.json"


class GradientEnhancedPredictor(nn.Module):
    """DDG predictor using VAE embeddings + discovered gradient direction."""

    def __init__(self, vae: DDGVAE, gradient_direction: torch.Tensor, dropout: float = 0.2):
        super().__init__()

        # Freeze VAE encoder
        self.encoder = vae.encoder
        for param in self.encoder.parameters():
            param.requires_grad = False

        # Register gradient direction as buffer (not parameter)
        self.register_buffer("gradient_direction", gradient_direction)

        # Learnable gradient projection refinement
        self.gradient_scale = nn.Parameter(torch.ones(1))
        self.gradient_bias = nn.Parameter(torch.zeros(1))

        # Residual MLP for corrections beyond gradient
        self.residual_mlp = nn.Sequential(
            nn.Linear(32, 32),
            nn.SiLU(),
            nn.LayerNorm(32),
            nn.Dropout(dropout),
            nn.Linear(32, 16),
            nn.SiLU(),
            nn.Linear(16, 1),
        )

        # Residual weight (how much to trust gradient vs MLP)
        self.residual_weight = nn.Parameter(torch.tensor(0.3))

    def forward(self, x: torch.Tensor) -> dict:
        """Forward pass.

        Args:
            x: Input features [batch, 20]

        Returns:
            Dict with ddg prediction and components
        """
        # Get VAE embedding
        with torch.no_grad():
            mu, _ = self.encoder(x)

        # Project onto gradient direction
        gradient_pred = torch.sum(mu * self.gradient_direction, dim=-1)
        gradient_pred = gradient_pred * self.gradient_scale + self.gradient_bias

        # Residual correction from MLP
        residual = self.residual_mlp(mu).squeeze(-1)

        # Combine with learned weight
        weight = torch.sigmoid(self.residual_weight)
        ddg = (1 - weight) * gradient_pred + weight * residual

        return {
            "ddg": ddg,
            "gradient_pred": gradient_pred,
            "residual": residual,
            "weight": weight,
            "embedding": mu,
        }


class GradientDataset(Dataset):
    """Dataset for gradient-enhanced prediction."""

    def __init__(self, records: list):
        self.features = []
        self.ddg_values = []

        for record in records:
            feat = compute_features(record.wild_type, record.mutant)
            # Pad to 20 dims for ProTherm encoder
            arr = feat.to_array(include_hyperbolic=False)
            arr = np.pad(arr, (0, 6), mode='constant')

            self.features.append(arr)
            self.ddg_values.append(record.ddg)

        self.features = np.array(self.features, dtype=np.float32)
        self.ddg_values = np.array(self.ddg_values, dtype=np.float32)

    def __len__(self):
        return len(self.ddg_values)

    def __getitem__(self, idx):
        return {
            "x": torch.from_numpy(self.features[idx]),
            "ddg": torch.tensor(self.ddg_values[idx], dtype=torch.float32),
        }


def train_gradient_enhanced():
    """Train gradient-enhanced predictor."""
    print("=" * 70)
    print("Gradient-Enhanced DDG Prediction")
    print("=" * 70)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    config = GradientConfig()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(f"outputs/gradient_enhanced_{timestamp}")
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"Output: {output_dir}")

    # Load gradient direction from discovery
    print("\n[1] Loading gradient direction...")
    with open(config.gradient_results) as f:
        gradient_results = json.load(f)

    gradient_direction = torch.tensor(
        gradient_results["gradient_analysis"]["gradient_direction"],
        dtype=torch.float32
    ).to(device)
    gradient_corr = gradient_results["gradient_analysis"]["ddg_correlation"]
    print(f"  Gradient correlation: {gradient_corr:.4f}")

    # Load VAE
    print("\n[2] Loading VAE-ProTherm...")
    vae = DDGVAE.create_protherm_variant(use_hyperbolic=False)
    ckpt = torch.load(config.vae_checkpoint, map_location=device, weights_only=False)
    vae.load_state_dict(ckpt["model_state_dict"])
    vae = vae.to(device)
    print("  VAE loaded")

    # Create model
    print("\n[3] Creating gradient-enhanced predictor...")
    model = GradientEnhancedPredictor(vae, gradient_direction, config.dropout)
    model = model.to(device)

    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  Trainable parameters: {trainable:,}")

    # Load data
    print("\n[4] Loading ProTherm data...")
    loader = ProThermLoader()
    db = loader.load_curated()
    records = db.records
    print(f"  Loaded {len(records)} mutations")

    # Train/val split
    np.random.seed(42)
    indices = np.random.permutation(len(records))
    split = int(0.8 * len(records))
    train_records = [records[i] for i in indices[:split]]
    val_records = [records[i] for i in indices[split:]]
    print(f"  Train: {len(train_records)}, Val: {len(val_records)}")

    train_dataset = GradientDataset(train_records)
    val_dataset = GradientDataset(val_records)

    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)

    # Optimizer
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
    )

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=config.epochs, eta_min=1e-6
    )

    # Training
    print("\n" + "=" * 70)
    print("[5] Training")
    print("=" * 70)

    best_spearman = -1
    patience_counter = 0
    history = {"train_loss": [], "val_loss": [], "val_spearman": [], "val_pearson": [], "weight": []}

    for epoch in range(config.epochs):
        # Train
        model.train()
        train_losses = []

        for batch in train_loader:
            x = batch["x"].to(device)
            ddg = batch["ddg"].to(device)

            optimizer.zero_grad()
            out = model(x)

            # Simple MSE loss
            loss = F.mse_loss(out["ddg"], ddg)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            train_losses.append(loss.item())

        # Validate
        model.eval()
        val_losses = []
        all_preds = []
        all_targets = []
        all_gradient_preds = []

        with torch.no_grad():
            for batch in val_loader:
                x = batch["x"].to(device)
                ddg = batch["ddg"].to(device)

                out = model(x)
                loss = F.mse_loss(out["ddg"], ddg)

                val_losses.append(loss.item())
                all_preds.extend(out["ddg"].cpu().numpy())
                all_targets.extend(ddg.cpu().numpy())
                all_gradient_preds.extend(out["gradient_pred"].cpu().numpy())

        # Metrics
        train_loss = np.mean(train_losses)
        val_loss = np.mean(val_losses)
        val_spearman = spearmanr(all_targets, all_preds)[0]
        val_pearson = pearsonr(all_targets, all_preds)[0]
        gradient_only_spearman = spearmanr(all_targets, all_gradient_preds)[0]
        weight = torch.sigmoid(model.residual_weight).item()

        history["train_loss"].append(float(train_loss))
        history["val_loss"].append(float(val_loss))
        history["val_spearman"].append(float(val_spearman))
        history["val_pearson"].append(float(val_pearson))
        history["weight"].append(float(weight))

        scheduler.step()

        # Logging
        if epoch % 10 == 0 or val_spearman > best_spearman:
            print(f"Epoch {epoch:3d}: loss={train_loss:.4f} val_loss={val_loss:.4f} "
                  f"spearman={val_spearman:.4f} pearson={val_pearson:.4f} "
                  f"grad_only={gradient_only_spearman:.4f} weight={weight:.3f}")

        # Best checkpoint
        if val_spearman > best_spearman:
            best_spearman = val_spearman
            best_pearson = val_pearson
            patience_counter = 0

            torch.save({
                "model_state_dict": model.state_dict(),
                "epoch": epoch,
                "val_spearman": val_spearman,
                "val_pearson": val_pearson,
                "gradient_direction": gradient_direction.cpu().numpy().tolist(),
                "config": config.__dict__,
            }, output_dir / "best.pt")
        else:
            patience_counter += 1
            if patience_counter >= config.patience:
                print(f"\nEarly stopping at epoch {epoch}")
                break

    # Save history
    with open(output_dir / "training_history.json", "w") as f:
        json.dump(history, f, indent=2)

    # Final results
    print("\n" + "=" * 70)
    print("GRADIENT-ENHANCED TRAINING COMPLETE")
    print("=" * 70)
    print(f"\nBest Spearman: {best_spearman:.4f}")
    print(f"Best Pearson:  {best_pearson:.4f}")
    print(f"Checkpoint: {output_dir / 'best.pt'}")

    print("\n" + "=" * 70)
    print("Results Comparison")
    print("=" * 70)
    print(f"  VAE-ProTherm alone:      0.64")
    print(f"  MLP Refiner:             0.78")
    print(f"  Gradient direction only: 0.95 (on full data)")
    print(f"  Gradient-Enhanced:       {best_spearman:.2f}")

    improvement = (best_spearman - 0.64) / 0.64 * 100
    print(f"\n  Improvement over VAE: {improvement:+.1f}%")

    if best_spearman >= 0.80:
        print("\n  TARGET ACHIEVED: Spearman >= 0.80")

    return model, best_spearman


if __name__ == "__main__":
    train_gradient_enhanced()
