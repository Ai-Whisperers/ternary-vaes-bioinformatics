# DDG VAE-Wide Configuration
# ==============================================
# Configuration for training the Wide diversity specialist VAE.
#
# Target: Learn diverse representations from large-scale data.
# Data: ProteinGym + ThermoMutDB + FireProtDB (500K+ mutations)
# Expected Spearman: ~0.50-0.60 (large-scale learning)

# Inherit from base
defaults:
  - base

# Training parameters for large-scale data
training:
  epochs: 50  # Fewer epochs for large data
  batch_size: 128  # Larger batch for efficiency
  learning_rate: 1.0e-3  # Faster for diverse data
  weight_decay: 1.0e-4  # More regularization

  grad_clip: 0.5  # Stronger clipping for stability
  early_stopping_patience: 10

  # No warmup for large data
  beta_schedule: constant

# Wide specialist architecture (maximum capacity)
model:
  input_dim: 14
  hidden_dim: 256  # Large capacity
  latent_dim: 64   # Large latent for diversity
  n_layers: 3      # Deeper for complex patterns

  activation: silu
  dropout: 0.15  # More dropout for diverse data
  use_layer_norm: true

  # Hyperbolic features
  use_hyperbolic: true
  hyperbolic_dim: 4

  # Higher KL weight for regularization
  beta: 2.0

# Validation strategy
validation:
  val_ratio: 0.1  # Smaller val ratio for large data
  cv_folds: 5

# Data settings
data:
  dataset: proteingym
  max_samples: 100000  # Limit for memory
  single_point_only: true
  include_thermomutdb: true
  include_fireprotdb: true

# Output
output:
  model_name: vae_wide
  experiment_dir: ${output.base_dir}/vae_wide

# Hardware optimization for large data
hardware:
  mixed_precision: true  # Faster training
  num_workers: 4  # Parallel data loading
