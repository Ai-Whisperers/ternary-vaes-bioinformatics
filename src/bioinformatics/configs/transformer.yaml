# DDG Transformer Configuration
# ==============================================
# Configuration for training transformer heads.
# Memory-optimized for RTX 3050 (6GB VRAM).
#
# Target: Precise predictions from full protein sequences.
# Expected Spearman: ~0.62+ with full-sequence, ~0.65+ hierarchical

# Inherit from base
defaults:
  - base

# Training parameters for transformers
training:
  epochs: 50
  batch_size: 4  # Small for memory
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  grad_clip: 1.0

  # Gradient accumulation for effective larger batch
  accumulation_steps: 8  # Effective batch = 32

  # Warmup for transformer stability
  warmup_epochs: 5

  early_stopping_patience: 15

# Hardware optimization (critical for 6GB VRAM)
hardware:
  device: cuda
  mixed_precision: true  # FP16 for memory savings
  num_workers: 2

# Full-sequence transformer
full_sequence:
  max_seq_len: 256  # Reduced for memory
  vocab_size: 22    # 20 AA + gap + mask

  d_model: 128      # Reduced from 256
  n_heads: 4
  n_layers: 3
  d_ff: 512
  dropout: 0.1

  # Memory optimization
  use_gradient_checkpointing: true
  use_flash_attention: false

  # Pre-LayerNorm for stability
  use_pre_layernorm: true
  use_gated_mlp: true

# Hierarchical transformer
hierarchical:
  max_seq_len: 256

  # Local encoder (mutation context)
  local_window: 21  # Â±10 residues around mutation

  # Global encoder (strided sequence)
  stride: 4  # Every 4th residue

  d_model: 128
  n_heads: 4
  n_layers: 3
  dropout: 0.1

  use_gradient_checkpointing: true

# Validation
validation:
  val_ratio: 0.2
  metric: val_spearman

# Output
output:
  model_name: ddg_transformer
  experiment_dir: ${output.base_dir}/transformer
