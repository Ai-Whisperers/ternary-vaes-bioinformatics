# DDG Multimodal VAE Configuration
# ==============================================
# Configuration for training the multimodal fusion VAE that
# combines embeddings from all three specialist VAEs.
#
# Target: Outperform individual specialists through fusion.
# Expected Spearman: ~0.55+ on S669

# Inherit from base
defaults:
  - base

# Training parameters for multimodal fusion
training:
  epochs: 100
  batch_size: 32
  learning_rate: 1.0e-4
  weight_decay: 1.0e-5
  grad_clip: 1.0

  early_stopping_patience: 20

  # Multi-task loss weights
  consistency_weight: 0.1  # Cross-encoder agreement
  ranking_weight: 0.1      # Pairwise DDG ordering

# Specialist VAE checkpoints (required)
specialists:
  s669:
    checkpoint: checkpoints/bioinformatics/vae_s669/best.pt
    latent_dim: 16
    freeze: true

  protherm:
    checkpoint: checkpoints/bioinformatics/vae_protherm/best.pt
    latent_dim: 32
    freeze: true

  wide:
    checkpoint: checkpoints/bioinformatics/vae_wide/best.pt
    latent_dim: 64
    freeze: true

# Fusion architecture
fusion:
  # Output dimension after fusion
  fused_dim: 128

  # Fusion type: concat, attention, gated
  fusion_type: attention
  n_heads: 8
  attention_dropout: 0.1

# Decoder architecture
decoder:
  hidden_dim: 256
  n_layers: 2
  dropout: 0.1

# VAE parameters
model:
  beta: 1.0
  logvar_min: -10.0
  logvar_max: 2.0

# Validation
validation:
  val_ratio: 0.2
  metric: val_spearman

# Output
output:
  model_name: multimodal_ddg
  experiment_dir: ${output.base_dir}/multimodal
